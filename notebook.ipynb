{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/krishna-sharma19/sparcs/blob/master/notebook.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "XXfUVO7V3I4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn import preprocessing    \n",
        "import tensorflow as tf\n",
        "from SALib.analyze import delta\n",
        "from SALib.util import read_param_file\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def cleanAndProcessData():\n",
        "    patient_data_df = pd.read_csv('../input/Hospital_Inpatient_Discharges__SPARCS_De-Identified___2015.csv')\n",
        "    X_Full = patient_data_df[['Age Group', 'Gender', 'Race', 'Ethnicity', 'Type of Admission','CCS Diagnosis Code','CCS Procedure Code',  'APR DRG Code', 'APR MDC Code','APR Severity of Illness Code','APR Risk of Mortality']]\n",
        "    pd.options.mode.chained_assignment = None\n",
        "    X_Full['Age Group']=pd.factorize(X_Full['Age Group'])[0]\n",
        "    X_Full['APR Risk of Mortality']=pd.factorize(X_Full['APR Risk of Mortality'])[0]\n",
        "    X_Full=pd.get_dummies(X_Full, columns=[\"Race\"])\n",
        "    X_Full=pd.get_dummies(X_Full, columns=[\"Ethnicity\"])\n",
        "    X_Full=pd.get_dummies(X_Full, columns=[\"Type of Admission\"])\n",
        "    X_Full=pd.get_dummies(X_Full, columns=[\"Gender\"])\n",
        "    Y_Full = patient_data_df[['Length of Stay']]\n",
        "    Y_Full = Y_Full.replace('120 +', 120)\n",
        "    Y_Full['Length of Stay'] = Y_Full['Length of Stay'].apply(pd.to_numeric)\n",
        "\n",
        "    Si = delta.analyze(problem, X_Full, Y_Full, num_resamples=10, conf_level=0.95, print_to_console=False)\n",
        "    print(str(Si['delta']))\n",
        "    return (patient_data_df,X_Full,Y_Full)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rr_6YxYX3I4d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def randomSample(patient_data_df,X_Full,Y_Full):\n",
        "    msk = np.random.rand(len(patient_data_df)) < 0.8\n",
        "    X_training = X_Full[msk]\n",
        "    X_testing = X_Full[~msk]\n",
        "\n",
        "    Y_training = Y_Full[msk]\n",
        "    Y_testing = Y_Full[~msk]\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # tf.reset_default_graph()\n",
        "    X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    # Scale both the training inputs and outputs\n",
        "    X_scaled_training = X_scaler.fit_transform(X_training)\n",
        "    Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
        "\n",
        "    # It's very important that the training and test data are scaled with the same scaler.\n",
        "    X_scaled_testing = X_scaler.transform(X_testing)\n",
        "    Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
        "    return (Y_scaler,Y_testing,(X_scaled_training,Y_scaled_training),(X_scaled_testing ,Y_scaled_testing))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fpAtm7CD3I4l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def neuralTraining(patient_data_df,X,Y,learning_rate = 0.001,training_epochs = 100, layer_1_nodes = 100, layer_2_nodes = 250, layer_3_nodes = 250 ,layer_4_nodes = 100):\n",
        "    (Y_scaler,Y_testing,(X_scaled_training, Y_scaled_training), (X_scaled_testing, Y_scaled_testing)) = randomSample(patient_data_df,X,Y)\n",
        "    # Define how many inputs and outputs are in our neural network\n",
        "    number_of_inputs = 24\n",
        "    number_of_outputs = 1\n",
        "\n",
        "    # Section One: Define the layers of the neural network itself\n",
        "\n",
        "    # Input Layer\n",
        "    tf.reset_default_graph() \n",
        "    with tf.variable_scope('input'):\n",
        "        X = tf.placeholder(tf.float32, shape=(None, number_of_inputs))\n",
        "\n",
        "    # Layer 1\n",
        "    with tf.variable_scope('layer_1'):\n",
        "        weights = tf.get_variable(\"weights1\", shape=[number_of_inputs, layer_1_nodes],\n",
        "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
        "        biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n",
        "        layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
        "\n",
        "    # Layer 2\n",
        "    with tf.variable_scope('layer_2'):\n",
        "        weights = tf.get_variable(\"weights2\", shape=[layer_1_nodes, layer_2_nodes],\n",
        "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
        "        biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
        "        layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
        "\n",
        "    # Layer 3\n",
        "    with tf.variable_scope('layer_3'):\n",
        "        weights = tf.get_variable(\"weights3\", shape=[layer_2_nodes, layer_3_nodes],\n",
        "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
        "        biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
        "        layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
        "\n",
        "    # Layer 4\n",
        "    with tf.variable_scope('layer_4'):\n",
        "        weights = tf.get_variable(\"weights4\", shape=[layer_3_nodes, layer_4_nodes],\n",
        "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
        "        biases = tf.get_variable(name=\"biases4\", shape=[layer_4_nodes], initializer=tf.zeros_initializer())\n",
        "        layer_4_output = tf.nn.relu(tf.matmul(layer_3_output, weights) + biases)\n",
        "\n",
        "    # Output Layer\n",
        "    with tf.variable_scope('output'):\n",
        "        weights = tf.get_variable(\"weights5\", shape=[layer_4_nodes, number_of_outputs],\n",
        "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
        "        biases = tf.get_variable(name=\"biases5\", shape=[number_of_outputs], initializer=tf.zeros_initializer())\n",
        "        prediction = tf.matmul(layer_4_output, weights) + biases\n",
        "\n",
        "    # Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
        "\n",
        "    with tf.variable_scope('cost'):\n",
        "        Y = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n",
        "\n",
        "    # Section Three: Define the optimizer function that will be run to optimize the neural network\n",
        "\n",
        "    with tf.variable_scope('train'):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "    # Create a summary operation to log the progress of the network\n",
        "    with tf.variable_scope('logging'):\n",
        "        tf.summary.scalar('current_cost', cost)\n",
        "        summary = tf.summary.merge_all()\n",
        "\n",
        "    rmsds= []\n",
        "    arr = np.asarray(Y_testing['Length of Stay'])\n",
        "    arr = np.transpose(np.asmatrix(arr))\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as session:\n",
        "        # When loading from a checkpoint, don't initialize the variables!\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        for epoch in range(training_epochs):\n",
        "\n",
        "            # Feed in the training data and do one step of neural network training\n",
        "            session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
        "\n",
        "            # Every 5 training steps, log our progress\n",
        "            if epoch % 3 == 0:\n",
        "                # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
        "                training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y:Y_scaled_training})\n",
        "                testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y:Y_scaled_testing})\n",
        "\n",
        "                Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
        "\n",
        "                Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
        "\n",
        "                rmsd = np.sqrt(np.mean(np.asarray((arr - Y_predicted)) ** 2))\n",
        "                Y_predicted = Y_predicted.astype(np.int64, copy=False)\n",
        "                import csv\n",
        "                with open(r'stats.cxv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([learning_rate,training_epochs, layer_1_nodes, layer_2_nodes, layer_3_nodes  ,layer_4_nodes,rmsd])\n",
        "\n",
        "\n",
        "                # Print the current training status to the screen\n",
        "                print(\"Epoch: {} - Training Cost: {}  Testing Cost: {} RMSD: {}\".format(epoch, training_cost, testing_cost, rmsd))\n",
        "\n",
        "            # Training is now complete!\n",
        "\n",
        "            # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
        "            final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
        "            final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n",
        "\n",
        "        print(\"Final Training cost: {}\".format(final_training_cost))\n",
        "        print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
        "\n",
        "        # print(arr)\n",
        "        # print(Y_predicted)\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # np.histogram(arr-Y_predicted)\n",
        "        plt.hist(arr - Y_predicted)\n",
        "        plt.show()\n",
        "        import csv\n",
        "        with open(r'stats.csv', 'a') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([learning_rate,training_epochs, layer_1_nodes, layer_2_nodes, layer_3_nodes  ,layer_4_nodes,rmsd])\n",
        "\n",
        "\n",
        "patient_data_df,X,Y = cleanAndProcessData();\n",
        "# for i in range(25,500,25):\n",
        "#     for j in range(i,500,25):\n",
        "#         for k in range(j,500,25):\n",
        "#             for l in range(25,j,25):\n",
        "#                 print(\"layer 1 nodes\",i,j,k,l)\n",
        "#                 neuralTraining(patient_data_df,X,Y,learning_rate = 0.01,training_epochs = 15, layer_1_nodes = 100, layer_2_nodes = 150, layer_3_nodes = 150 ,layer_4_nodes = 100)\n",
        "# import sys\n",
        "\n",
        "\n",
        "\n",
        "# sys.path.append('../..')\n",
        "\n",
        "\n",
        "# Read the parameter range file and generate samples\n",
        "# Since this is \"given data\", the bounds in the parameter file will not be used\n",
        "# but the columns are still expected\n",
        "# problem = read_param_file('../../SALib/test_functions/params/Ishigami.txt')\n",
        "# X = np.loadtxt('model_input.txt')\n",
        "# Y = np.loadtxt('model_output.txt')\n",
        "\n",
        "# Perform the sensitivity analysis using the model output\n",
        "# Specify which column of the output file to analyze (zero-indexed)\n",
        "# Si = delta.analyze(problem, X, Y, num_resamples=10, conf_level=0.95, print_to_console=False)\n",
        "# Returns a dictionary with keys 'delta', 'delta_conf', 'S1', 'S1_conf'\n",
        "# print(str(Si['delta']))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}